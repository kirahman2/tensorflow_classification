{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "filepath_files = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_samples = pd.read_csv(filepath_files + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_labels = pd.read_csv(filepath_files + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_samples = pd.concat([train_samples, train_labels], axis=1)\n",
    "\n",
    "test_samples = pd.read_csv(filepath_files + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_labels = pd.read_csv(filepath_files + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_samples = pd.concat([test_samples, test_labels], axis=1)\n",
    "\n",
    "test_samples = shuffle(test_samples).reset_index(drop=True)\n",
    "\n",
    "neg, pos = np.bincount(train_samples['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "test_samples, val_samples = train_test_split(test_samples, test_size=.2)\n",
    "\n",
    "train_labels = np.array(train_samples.pop('0'))\n",
    "test_labels = np.array(test_samples.pop('0'))\n",
    "val_labels = np.array(val_samples.pop('0'))\n",
    "\n",
    "print(\"Training data shape:\", train_samples.shape)\n",
    "print(\"Validation data shape:\", val_samples.shape)\n",
    "print(\"Testing data shape:\", test_samples.shape)\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "val_samples = np.array(val_samples)\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = StandardScaler()\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "scaled_val_samples = scaler.transform(val_samples)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "\n",
    "# test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "# list_data = [train_sample, test_sample]\n",
    "\n",
    "\n",
    "\n",
    "# class PreProcessing():\n",
    "#     def __init__(self, list_data):\n",
    "# #         self._parse_list_data(list_data)\n",
    "#         self.process_data(list_data)\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "\n",
    "#         # NEXT, call the self.val_sample here have it return from split. \n",
    "# #     def _parse_list_data(self, list_data):\n",
    "# #         self.train_sample = list_data[0]\n",
    "# #         self.test_sample = list_data[1]\n",
    "        \n",
    "#     def process_data(self, list_data):\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "#         self._shuffle()\n",
    "#         self._split_test_data()\n",
    "#         self._create_target()\n",
    "#         self._training_sets_array()\n",
    "#         self._scale_data()\n",
    "        \n",
    "#     def _shuffle(self):\n",
    "#         self.test_sample = shuffle(self.test_sample).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     def _split_test_data(self):\n",
    "#         self.test_sample, self.val_sample = train_test_split(self.test_sample,\n",
    "#                                                              test_size=.2)\n",
    "        \n",
    "#     def _create_target(self):\n",
    "#         self.train_label = np.array(self.train_sample.pop('0'))\n",
    "#         self.test_label = np.array(self.test_sample.pop('0'))\n",
    "#         self.val_label = np.array(self.val_sample.pop('0'))\n",
    "#         self._print_summary()\n",
    "        \n",
    "#     def _print_summary(self):\n",
    "#         print(\"Training data shape:\", self.train_sample.shape)\n",
    "#         print(\"Validation data shape:\", self.val_sample.shape)\n",
    "#         print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        \n",
    "#     def _training_sets_array(self):\n",
    "#         self.train_sample = np.array(self.train_sample)\n",
    "#         self.test_sample = np.array(self.test_sample)\n",
    "#         self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "#     def _scale_data(self):\n",
    "#         scaler = StandardScaler()\n",
    "#         self.scaled_train_sample = scaler.fit_transform(self.train_sample)\n",
    "#         self.scaled_test_sample = scaler.transform(self.test_sample)\n",
    "#         self.scaled_val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "# pp = PreProcessing(list_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (190000, 233)\n",
      "Testing data shape: (47243, 233)\n",
      "Validation data shape: (11811, 233)\n",
      "train_label length: 190000\n",
      "test_label length: 47243\n",
      "val_label length: 11811\n"
     ]
    }
   ],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "target = '0'\n",
    "\n",
    "list_data = [train_sample, test_sample]\n",
    "\n",
    "class PreProcessing():\n",
    "    def __init__(self, list_data, target):\n",
    "        self.target = target\n",
    "        self.train_sample = list_data[0]\n",
    "        self.test_sample = list_data[1]\n",
    "        self.test_sample = self.shuffle_data(self.test_sample)\n",
    "        self.test_sample, self.val_sample = self._split_test_data()\n",
    "        self.train_label = self._create_target(self.train_sample)\n",
    "        self.test_label = self._create_target(self.test_sample)\n",
    "        self.val_label = self._create_target(self.val_sample)\n",
    "        self.process_data()\n",
    "        \n",
    "    def process_data(self):\n",
    "        self._print_summary()\n",
    "        self._training_sets_array()\n",
    "        self._scale_data()\n",
    "        \n",
    "    def shuffle_data(self, dataset):\n",
    "        return shuffle(dataset).reset_index(drop=True)\n",
    "    \n",
    "    def _split_test_data(self):\n",
    "        return train_test_split(self.test_sample, test_size=.2)\n",
    "        \n",
    "    def _create_target(self, dataset):\n",
    "        return np.array(dataset.pop(self.target))\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        print(\"Training data shape:\", self.train_sample.shape)\n",
    "        print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        print(\"Validation data shape:\", self.val_sample.shape)\n",
    "        print(\"train_label length:\", self.train_label.shape[0])\n",
    "        print(\"test_label length:\", self.test_label.shape[0])\n",
    "        print(\"val_label length:\", self.val_label.shape[0])\n",
    "        \n",
    "    def _training_sets_array(self):\n",
    "        self.train_sample = np.array(self.train_sample)\n",
    "        self.test_sample = np.array(self.test_sample)\n",
    "        self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train_sample = scaler.fit_transform(self.train_sample)\n",
    "        self.test_sample = scaler.transform(self.test_sample)\n",
    "        self.val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "pp = PreProcessing(list_data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a method to load the algorithm\n",
    "# create method to run through epochs\n",
    "# create method to test each threshold .1-.9 and save all results to a dataframe\n",
    "# create method that reads dataframe and selects which has the highest auc roc score and returns the best \n",
    "# threshold\n",
    "# create method using code block below that selects each threshold and creates the fine tuning set, which then\n",
    "# tests each and selects the perfect threshold to use\n",
    "# create method that saves the new model in tf SavedModel format\n",
    "# create method that creates results dataframe along with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append each val to list and then call order command to order the list \n",
    "# Method for fine tuning threshold\n",
    "list_tune_thres = []\n",
    "\n",
    "best_thres = .1\n",
    "temp_val_neg = .1\n",
    "temp_val_pos = .1\n",
    "for val in range(0,5):\n",
    "    temp_val_neg = temp_val_neg - .01\n",
    "    temp_val_pos = temp_val_pos + .01\n",
    "    list_tune_thres.append(round(temp_val_neg, 2))\n",
    "    list_tune_thres.append(round(temp_val_pos, 2))\n",
    "list_tune_thres.append(round(best_thres, 2))    \n",
    "    \n",
    "list_tune_thres.sort()\n",
    "list_tune_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score: 0.8575535043722968 0.31\n",
      "auc_score: 0.8583632997127213 0.32\n",
      "auc_score: 0.8584245742529265 0.33\n",
      "auc_score: 0.8587174464479711 0.34\n",
      "auc_score: 0.8596040601896949 0.35\n",
      "auc_score: 0.8597652436967979 0.36\n",
      "auc_score: 0.8610711565188865 0.37\n",
      "auc_score: 0.8607471966016537 0.38\n",
      "auc_score: 0.8602366777098369 0.39\n",
      "auc_score: 0.8606404329291076 0.4\n",
      "auc_score: 0.8606150574616265 0.41\n",
      "auc_score: 0.8609529683369265 0.42\n",
      "auc_score: 0.860332708871825 0.43\n",
      "auc_score: 0.8601635282081321 0.44\n",
      "auc_score: 0.8599175291431402 0.45\n",
      "auc_score: 0.8596386079061626 0.46\n"
     ]
    }
   ],
   "source": [
    "# use this in method to run scoring on \n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "\n",
    "list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "                  .35, .4, .45, .5, .55, .6]\n",
    "\n",
    "list_threshold = [.31, .32, .33, .34, .35, .36, \n",
    "                  .37, .38, .39, .4, .41, .42,.43,.44,.45,.46]\n",
    "for threshold in list_threshold:\n",
    "    y_pred_class = binarize(y_pred, threshold)#[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, y_pred_class, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    # list_auc_score.append(auc_score)\n",
    "    print('auc_score:', auc_score, threshold)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _predict_proba_threshold(self, y_pred):\n",
    "#         '''create prediction for each threshold'''\n",
    "#         df_results = pd.DataFrame()\n",
    "#         list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "#                           .35, .4, .45, .5, .55, .6]\n",
    "#         for threshold in list_threshold:\n",
    "#             df_temp = self._compute_thres_df(y_pred, threshold)\n",
    "#             df_results = pd.concat([df_results, df_temp], axis=0)\n",
    "#         df_results = df_results.drop('index', axis=1).reset_index(drop=True)\n",
    "#         val_thres_cost, val_thres_auc = self._calc_and_plot_results(df_results)\n",
    "#         y_pred_class = self._y_pred_class(y_pred, val_thres_cost, val_thres_auc)\n",
    "#         print('\\nBelow are the results by threshold:\\n')\n",
    "#         print(df_results)\n",
    "#         return y_pred_class\n",
    "    \n",
    "# def _compute_thres_df(self, y_pred_prob, threshold):\n",
    "#         '''compute the values for each threshold'''\n",
    "#         y_pred_class = binarize(y_pred_prob, threshold)[:,1]\n",
    "#         df_conf_matrix = self._compute_conf_matrix(y_pred_class)\n",
    "#         df_dict_results = self._compute_results(df_conf_matrix, y_pred_class, threshold)\n",
    "#         return df_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score: 0.8592748711112961\n",
      "[[42225  3337]\n",
      " [  350  1331]]\n"
     ]
    }
   ],
   "source": [
    "# Final model WORKING\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "# predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "neg, pos = np.bincount(train_samples['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT, should we load model weights and use this as our method? \n",
    "# should we just load the model? Well, we do want to show them all our details, so we should do this all.\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 30\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model - for non OOP dataset\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "# val_epoch = val_epoch + 1\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 9\n",
    "for neurons in [256]:\n",
    "    print('4 layer, neurons:', neurons)\n",
    "    metrics = [keras.metrics.AUC(name='auc'),\n",
    "               keras.metrics.FalsePositives(name='fp'),\n",
    "               keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "    epochs = 9\n",
    "    lr = .001\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(neurons, activation='relu', input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "              batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "              use_multiprocessing=True)\n",
    "\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    print('auc score:\\n', auc_score)\n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.\n",
    "# we need to graph the training, validation and test results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 55\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
