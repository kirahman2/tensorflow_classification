{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "filepath_files = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_samples = pd.read_csv(filepath_files + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_labels = pd.read_csv(filepath_files + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_samples = pd.concat([train_samples, train_labels], axis=1)\n",
    "\n",
    "test_samples = pd.read_csv(filepath_files + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_labels = pd.read_csv(filepath_files + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_samples = pd.concat([test_samples, test_labels], axis=1)\n",
    "\n",
    "test_samples = shuffle(test_samples).reset_index(drop=True)\n",
    "\n",
    "neg, pos = np.bincount(train_samples['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "test_samples, val_samples = train_test_split(test_samples, test_size=.2)\n",
    "\n",
    "train_labels = np.array(train_samples.pop('0'))\n",
    "test_labels = np.array(test_samples.pop('0'))\n",
    "val_labels = np.array(val_samples.pop('0'))\n",
    "\n",
    "print(\"Training data shape:\", train_samples.shape)\n",
    "print(\"Validation data shape:\", val_samples.shape)\n",
    "print(\"Testing data shape:\", test_samples.shape)\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "val_samples = np.array(val_samples)\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = StandardScaler()\n",
    "scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "scaled_test_samples = scaler.transform(test_samples)\n",
    "scaled_val_samples = scaler.transform(val_samples)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "\n",
    "# test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "# list_data = [train_sample, test_sample]\n",
    "\n",
    "\n",
    "\n",
    "# class PreProcessing():\n",
    "#     def __init__(self, list_data):\n",
    "# #         self._parse_list_data(list_data)\n",
    "#         self.process_data(list_data)\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "\n",
    "#         # NEXT, call the self.val_sample here have it return from split. \n",
    "# #     def _parse_list_data(self, list_data):\n",
    "# #         self.train_sample = list_data[0]\n",
    "# #         self.test_sample = list_data[1]\n",
    "        \n",
    "#     def process_data(self, list_data):\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "#         self._shuffle()\n",
    "#         self._split_test_data()\n",
    "#         self._create_target()\n",
    "#         self._training_sets_array()\n",
    "#         self._scale_data()\n",
    "        \n",
    "#     def _shuffle(self):\n",
    "#         self.test_sample = shuffle(self.test_sample).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     def _split_test_data(self):\n",
    "#         self.test_sample, self.val_sample = train_test_split(self.test_sample,\n",
    "#                                                              test_size=.2)\n",
    "        \n",
    "#     def _create_target(self):\n",
    "#         self.train_label = np.array(self.train_sample.pop('0'))\n",
    "#         self.test_label = np.array(self.test_sample.pop('0'))\n",
    "#         self.val_label = np.array(self.val_sample.pop('0'))\n",
    "#         self._print_summary()\n",
    "        \n",
    "#     def _print_summary(self):\n",
    "#         print(\"Training data shape:\", self.train_sample.shape)\n",
    "#         print(\"Validation data shape:\", self.val_sample.shape)\n",
    "#         print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        \n",
    "#     def _training_sets_array(self):\n",
    "#         self.train_sample = np.array(self.train_sample)\n",
    "#         self.test_sample = np.array(self.test_sample)\n",
    "#         self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "#     def _scale_data(self):\n",
    "#         scaler = StandardScaler()\n",
    "#         self.scaled_train_sample = scaler.fit_transform(self.train_sample)\n",
    "#         self.scaled_test_sample = scaler.transform(self.test_sample)\n",
    "#         self.scaled_val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "# pp = PreProcessing(list_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (190000, 233)\n",
      "Testing data shape: (47243, 233)\n",
      "Validation data shape: (11811, 233)\n",
      "train_label length: 190000\n",
      "test_label length: 47243\n",
      "val_label length: 11811\n"
     ]
    }
   ],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "target = '0'\n",
    "\n",
    "list_data = [train_sample, test_sample]\n",
    "\n",
    "class PreProcessing():\n",
    "    def __init__(self, list_data, target):\n",
    "        self.target = target\n",
    "        self.train_sample = list_data[0]\n",
    "        self.test_sample = list_data[1]\n",
    "        self.test_sample = self.shuffle_data(self.test_sample)\n",
    "        self.test_sample, self.val_sample = self._split_test_data()\n",
    "        self.train_label = self._create_target(self.train_sample)\n",
    "        self.test_label = self._create_target(self.test_sample)\n",
    "        self.val_label = self._create_target(self.val_sample)\n",
    "        self.process_data()\n",
    "        \n",
    "    def process_data(self):\n",
    "        self._print_summary()\n",
    "        self._training_sets_array()\n",
    "        self._scale_data()\n",
    "        \n",
    "    def shuffle_data(self, dataset):\n",
    "        return shuffle(dataset).reset_index(drop=True)\n",
    "    \n",
    "    def _split_test_data(self):\n",
    "        return train_test_split(self.test_sample, test_size=.2)\n",
    "        \n",
    "    def _create_target(self, dataset):\n",
    "        return np.array(dataset.pop(self.target))\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        print(\"Training data shape:\", self.train_sample.shape)\n",
    "        print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        print(\"Validation data shape:\", self.val_sample.shape)\n",
    "        print(\"train_label length:\", self.train_label.shape[0])\n",
    "        print(\"test_label length:\", self.test_label.shape[0])\n",
    "        print(\"val_label length:\", self.val_label.shape[0])\n",
    "        \n",
    "    def _training_sets_array(self):\n",
    "        self.train_sample = np.array(self.train_sample)\n",
    "        self.test_sample = np.array(self.test_sample)\n",
    "        self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train_sample = scaler.fit_transform(self.train_sample)\n",
    "        self.test_sample = scaler.transform(self.test_sample)\n",
    "        self.val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "pp = PreProcessing(list_data, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(pp.test_sample, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.04543716e-01],\n",
       "       [1.68702602e-02],\n",
       "       [9.99288142e-01],\n",
       "       ...,\n",
       "       [2.81370711e-03],\n",
       "       [5.99788063e-06],\n",
       "       [1.62345372e-04]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(pp.test_sample, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score: 0.8592748711112961\n",
      "[[42225  3337]\n",
      " [  350  1331]]\n"
     ]
    }
   ],
   "source": [
    "# Final model WORKING\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "# predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "neg, pos = np.bincount(train_samples['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT, should we load model weights and use this as our method? \n",
    "# should we just load the model? Well, we do want to show them all our details, so we should do this all.\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 30\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model - for non OOP dataset\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "# val_epoch = val_epoch + 1\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 9\n",
    "for neurons in [256]:\n",
    "    print('4 layer, neurons:', neurons)\n",
    "    metrics = [keras.metrics.AUC(name='auc'),\n",
    "               keras.metrics.FalsePositives(name='fp'),\n",
    "               keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "    epochs = 9\n",
    "    lr = .001\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(neurons, activation='relu', input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(neurons, activation='relu'),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "              batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "              use_multiprocessing=True)\n",
    "\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    print('auc score:\\n', auc_score)\n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.\n",
    "# we need to graph the training, validation and test results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 55\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
