{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete once we are sure that our preprocessing method is ready. \n",
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# filepath_files = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_samples = pd.read_csv(filepath_files + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_labels = pd.read_csv(filepath_files + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_samples = pd.concat([train_samples, train_labels], axis=1)\n",
    "\n",
    "# test_samples = pd.read_csv(filepath_files + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_labels = pd.read_csv(filepath_files + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_samples = pd.concat([test_samples, test_labels], axis=1)\n",
    "\n",
    "# test_samples = shuffle(test_samples).reset_index(drop=True)\n",
    "\n",
    "# neg, pos = np.bincount(train_samples['0'])\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# test_samples, val_samples = train_test_split(test_samples, test_size=.2)\n",
    "\n",
    "# train_labels = np.array(train_samples.pop('0'))\n",
    "# test_labels = np.array(test_samples.pop('0'))\n",
    "# val_labels = np.array(val_samples.pop('0'))\n",
    "\n",
    "# print(\"Training data shape:\", train_samples.shape)\n",
    "# print(\"Validation data shape:\", val_samples.shape)\n",
    "# print(\"Testing data shape:\", test_samples.shape)\n",
    "\n",
    "# train_samples = np.array(train_samples)\n",
    "# test_samples = np.array(test_samples)\n",
    "# val_samples = np.array(val_samples)\n",
    "\n",
    "# # scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler = StandardScaler()\n",
    "# scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# scaled_test_samples = scaler.transform(test_samples)\n",
    "# scaled_val_samples = scaler.transform(val_samples)\n",
    "\n",
    "# loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "\n",
    "# test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "# list_data = [train_sample, test_sample]\n",
    "\n",
    "\n",
    "\n",
    "# class PreProcessing():\n",
    "#     def __init__(self, list_data):\n",
    "# #         self._parse_list_data(list_data)\n",
    "#         self.process_data(list_data)\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "\n",
    "#         # NEXT, call the self.val_sample here have it return from split. \n",
    "# #     def _parse_list_data(self, list_data):\n",
    "# #         self.train_sample = list_data[0]\n",
    "# #         self.test_sample = list_data[1]\n",
    "        \n",
    "#     def process_data(self, list_data):\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "#         self._shuffle()\n",
    "#         self._split_test_data()\n",
    "#         self._create_target()\n",
    "#         self._training_sets_array()\n",
    "#         self._scale_data()\n",
    "        \n",
    "#     def _shuffle(self):\n",
    "#         self.test_sample = shuffle(self.test_sample).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     def _split_test_data(self):\n",
    "#         self.test_sample, self.val_sample = train_test_split(self.test_sample,\n",
    "#                                                              test_size=.2)\n",
    "        \n",
    "#     def _create_target(self):\n",
    "#         self.train_label = np.array(self.train_sample.pop('0'))\n",
    "#         self.test_label = np.array(self.test_sample.pop('0'))\n",
    "#         self.val_label = np.array(self.val_sample.pop('0'))\n",
    "#         self._print_summary()\n",
    "        \n",
    "#     def _print_summary(self):\n",
    "#         print(\"Training data shape:\", self.train_sample.shape)\n",
    "#         print(\"Validation data shape:\", self.val_sample.shape)\n",
    "#         print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        \n",
    "#     def _training_sets_array(self):\n",
    "#         self.train_sample = np.array(self.train_sample)\n",
    "#         self.test_sample = np.array(self.test_sample)\n",
    "#         self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "#     def _scale_data(self):\n",
    "#         scaler = StandardScaler()\n",
    "#         self.scaled_train_sample = scaler.fit_transform(self.train_sample)\n",
    "#         self.scaled_test_sample = scaler.transform(self.test_sample)\n",
    "#         self.scaled_val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "# pp = PreProcessing(list_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "...    ..\n",
       "189995  1\n",
       "189996  0\n",
       "189997  1\n",
       "189998  1\n",
       "189999  1\n",
       "\n",
       "[190000 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (190000, 233)\n",
      "Testing data shape: (47243, 233)\n",
      "Validation data shape: (11811, 233)\n",
      "train_label length: 190000\n",
      "test_label length: 47243\n",
      "val_label length: 11811\n"
     ]
    }
   ],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "target = '0'\n",
    "\n",
    "list_data = [train_sample, test_sample]\n",
    "\n",
    "class PreProcessing():\n",
    "    def __init__(self, list_data, target):\n",
    "        self.target = target\n",
    "        self.train_sample = list_data[0].copy()\n",
    "        self.test_sample = list_data[1].copy()\n",
    "        self.test_sample = self.shuffle_data(self.test_sample)\n",
    "        self.test_sample, self.val_sample = self._split_test_data()\n",
    "        self.train_label = self._create_target(self.train_sample)\n",
    "        self.test_label = self._create_target(self.test_sample)\n",
    "        self.val_label = self._create_target(self.val_sample)\n",
    "        self.process_data()\n",
    "        \n",
    "    def process_data(self):\n",
    "        self._print_summary()\n",
    "        self._training_sets_array()\n",
    "        self._scale_data()\n",
    "        \n",
    "    def shuffle_data(self, dataset):\n",
    "        return shuffle(dataset).reset_index(drop=True)\n",
    "    \n",
    "    def _split_test_data(self):\n",
    "        return train_test_split(self.test_sample, test_size=.2)\n",
    "        \n",
    "    def _create_target(self, dataset):\n",
    "        return np.array(dataset.pop(self.target))\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        print(\"Training data shape:\", self.train_sample.shape)\n",
    "        print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        print(\"Validation data shape:\", self.val_sample.shape)\n",
    "        print(\"train_label length:\", self.train_label.shape[0])\n",
    "        print(\"test_label length:\", self.test_label.shape[0])\n",
    "        print(\"val_label length:\", self.val_label.shape[0])\n",
    "        \n",
    "    def _training_sets_array(self):\n",
    "        self.train_sample = np.array(self.train_sample)\n",
    "        self.test_sample = np.array(self.test_sample)\n",
    "        self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train_sample = scaler.fit_transform(self.train_sample)\n",
    "        self.test_sample = scaler.transform(self.test_sample)\n",
    "        self.val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "pp = PreProcessing(list_data, target)\n",
    "# right now, train_sample is copied, so the original train_sample is not modified. We might need to reverse\n",
    "# our change later when it comes to feeding the model our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create method that ingests selected model, runs through 90 epochs, saves each model, graphs auc results for all\n",
    "# 90 models. then scores each based on \n",
    "# best .1-.9 threshold, then selects the best threshold for each epoch, then tests the best incremental threshold\n",
    "# for all 90 models. Then selects the top model, top threshold, then saves the model as a tf file. Then load model\n",
    "# and score it with results. \n",
    "# \n",
    "# then run through each threshold and picks the\n",
    "# top 5 scoring models and then runs through each threshold, \n",
    "\n",
    "# create method to test each threshold .1-.9 and save all results to a dataframe\n",
    "# create method that reads dataframe and selects which has the highest auc roc score and returns the best \n",
    "# threshold\n",
    "# create a method to load the algorithm\n",
    "\n",
    "# create method using code block below that selects each threshold and creates the fine tuning set, which then\n",
    "# tests each and selects the perfect threshold to use\n",
    "# create method that saves the new model in tf SavedModel format\n",
    "# create method that creates results dataframe along with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score: 0.8644288146005411\n",
      "[[42207  3368]\n",
      " [  329  1339]]\n"
     ]
    }
   ],
   "source": [
    "# Final model WORKING\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "# predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append each val to list and then call order command to order the list \n",
    "# Method for fine tuning threshold\n",
    "list_tune_thres = []\n",
    "\n",
    "best_thres = .1\n",
    "temp_val_neg = .1\n",
    "temp_val_pos = .1\n",
    "for val in range(0,5):\n",
    "    temp_val_neg = temp_val_neg - .01\n",
    "    temp_val_pos = temp_val_pos + .01\n",
    "    list_tune_thres.append(round(temp_val_neg, 2))\n",
    "    list_tune_thres.append(round(temp_val_pos, 2))\n",
    "list_tune_thres.append(round(best_thres, 2))    \n",
    "    \n",
    "list_tune_thres.sort()\n",
    "list_tune_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this in method to run scoring on \n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "\n",
    "list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "                  .35, .4, .45, .5, .55, .6]\n",
    "\n",
    "list_threshold = [.31, .32, .33, .34, .35, .36, \n",
    "                  .37, .38, .39, .4, .41, .42,.43,.44,.45,.46]\n",
    "for threshold in list_threshold:\n",
    "    y_pred_class = binarize(y_pred, threshold)#[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, y_pred_class, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    # list_auc_score.append(auc_score)\n",
    "    print('auc_score:', auc_score, threshold)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "neg, pos = np.bincount(train_sample['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from other machine. \n",
    "#8 inner .5 dropout .0001 l2\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "neg, pos = np.bincount(train_sample['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "\n",
    "##### model begins below ##### \n",
    "checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "epochs = 90\n",
    "lr = .0001\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_sample.shape[-1],)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "### Testing\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "### testing ^\n",
    "\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "clas Model():\n",
    "    def __init__(self, train_sample):\n",
    "        self.train_sample = train_sample\n",
    "    \n",
    "    def _class_imbalance(self):\n",
    "        neg, pos = np.bincount(train_sample[target])\n",
    "        initial_bias = np.log([pos/neg])\n",
    "        output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "        return\n",
    "    def _method(self):\n",
    "        return\n",
    "    \n",
    "model_class = Model(pp.train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190000 samples, validate on 11811 samples\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: saving model to ./cp.ckpt/cp-0001.ckpt\n",
      "190000/190000 - 223s - loss: 0.4536 - auc: 0.9433 - fp: 10341.0000 - fn: 13733.0000 - val_loss: 0.3658 - val_auc: 0.9119 - val_fp: 929.0000 - val_fn: 111.0000\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: saving model to ./cp.ckpt/cp-0002.ckpt\n",
      "190000/190000 - 188s - loss: 0.3181 - auc: 0.9726 - fp: 6508.0000 - fn: 9188.0000 - val_loss: 0.3028 - val_auc: 0.9276 - val_fp: 676.0000 - val_fn: 124.0000\n"
     ]
    }
   ],
   "source": [
    "# CURRENT!\n",
    "# This is the code we are using to create our model class. \n",
    "# NEXT, should we load model weights and use this as our method? \n",
    "# should we just load the model? Well, we do want to show them all our details, so we should do this all.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np \n",
    "\n",
    "neg, pos = np.bincount(train_sample['0'])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 2\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# fitting model\n",
    "model.fit(pp.train_sample, pp.train_label, validation_data=(pp.val_sample, pp.val_label), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "checkpoint_path = './cp.ckpt/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000 auc_score: 0.5229075644638919\n",
      "epoch: 0001 auc_score: 0.822263526314429\n",
      "epoch: 0002 auc_score: 0.8184907573037039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x167770dd0>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CURRENT!\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "    model.load_weights(filepath_chkpt)\n",
    "\n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    \n",
    "    chkpt_num = chkpt[3:7]\n",
    "    print('epoch/checkpoint:', chkpt_num, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 auc_score: 0.49883807235657746\n",
      "epoch: 2 auc_score: 0.7928097864286312\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch: 3 auc_score: 0.8104485288415588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15db984d0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    \n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model - for non OOP dataset\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "# val_epoch = val_epoch + 1\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 55\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
