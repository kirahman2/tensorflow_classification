{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.regularizers import l1, l2, L1L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete once we are sure that our preprocessing method is ready. \n",
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# filepath_files = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_samples = pd.read_csv(filepath_files + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_labels = pd.read_csv(filepath_files + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_samples = pd.concat([train_samples, train_labels], axis=1)\n",
    "\n",
    "# test_samples = pd.read_csv(filepath_files + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_labels = pd.read_csv(filepath_files + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_samples = pd.concat([test_samples, test_labels], axis=1)\n",
    "\n",
    "# test_samples = shuffle(test_samples).reset_index(drop=True)\n",
    "\n",
    "# neg, pos = np.bincount(train_samples['0'])\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# test_samples, val_samples = train_test_split(test_samples, test_size=.2)\n",
    "\n",
    "# train_labels = np.array(train_samples.pop('0'))\n",
    "# test_labels = np.array(test_samples.pop('0'))\n",
    "# val_labels = np.array(val_samples.pop('0'))\n",
    "\n",
    "# print(\"Training data shape:\", train_samples.shape)\n",
    "# print(\"Validation data shape:\", val_samples.shape)\n",
    "# print(\"Testing data shape:\", test_samples.shape)\n",
    "\n",
    "# train_samples = np.array(train_samples)\n",
    "# test_samples = np.array(test_samples)\n",
    "# val_samples = np.array(val_samples)\n",
    "\n",
    "# # scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler = StandardScaler()\n",
    "# scaled_train_samples = scaler.fit_transform(train_samples)\n",
    "# scaled_test_samples = scaler.transform(test_samples)\n",
    "# scaled_val_samples = scaler.transform(val_samples)\n",
    "\n",
    "# loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "# from sklearn.utils import shuffle\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "# train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "# train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "\n",
    "# test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "# test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "# test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "# list_data = [train_sample, test_sample]\n",
    "\n",
    "\n",
    "\n",
    "# class PreProcessing():\n",
    "#     def __init__(self, list_data):\n",
    "# #         self._parse_list_data(list_data)\n",
    "#         self.process_data(list_data)\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "\n",
    "#         # NEXT, call the self.val_sample here have it return from split. \n",
    "# #     def _parse_list_data(self, list_data):\n",
    "# #         self.train_sample = list_data[0]\n",
    "# #         self.test_sample = list_data[1]\n",
    "        \n",
    "#     def process_data(self, list_data):\n",
    "#         self.train_sample = list_data[0]\n",
    "#         self.test_sample = list_data[1]\n",
    "#         self._shuffle()\n",
    "#         self._split_test_data()\n",
    "#         self._create_target()\n",
    "#         self._training_sets_array()\n",
    "#         self._scale_data()\n",
    "        \n",
    "#     def _shuffle(self):\n",
    "#         self.test_sample = shuffle(self.test_sample).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     def _split_test_data(self):\n",
    "#         self.test_sample, self.val_sample = train_test_split(self.test_sample,\n",
    "#                                                              test_size=.2)\n",
    "        \n",
    "#     def _create_target(self):\n",
    "#         self.train_label = np.array(self.train_sample.pop('0'))\n",
    "#         self.test_label = np.array(self.test_sample.pop('0'))\n",
    "#         self.val_label = np.array(self.val_sample.pop('0'))\n",
    "#         self._print_summary()\n",
    "        \n",
    "#     def _print_summary(self):\n",
    "#         print(\"Training data shape:\", self.train_sample.shape)\n",
    "#         print(\"Validation data shape:\", self.val_sample.shape)\n",
    "#         print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        \n",
    "#     def _training_sets_array(self):\n",
    "#         self.train_sample = np.array(self.train_sample)\n",
    "#         self.test_sample = np.array(self.test_sample)\n",
    "#         self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "#     def _scale_data(self):\n",
    "#         scaler = StandardScaler()\n",
    "#         self.scaled_train_sample = scaler.fit_transform(self.train_sample)\n",
    "#         self.scaled_test_sample = scaler.transform(self.test_sample)\n",
    "#         self.scaled_val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "# pp = PreProcessing(list_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (190000, 233)\n",
      "Testing data shape: (47243, 233)\n",
      "Validation data shape: (11811, 233)\n",
      "train_label length: 190000\n",
      "test_label length: 47243\n",
      "val_label length: 11811\n"
     ]
    }
   ],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "target = '0'\n",
    "\n",
    "list_data = [train_sample, test_sample]\n",
    "\n",
    "class PreProcessing():\n",
    "    def __init__(self, list_data, target):\n",
    "        self.target = target\n",
    "        self.train_sample = list_data[0].copy()\n",
    "        self.test_sample = list_data[1].copy()\n",
    "        self.test_sample = self.shuffle_data(self.test_sample)\n",
    "        self.test_sample, self.val_sample = self._split_test_data()\n",
    "        self.train_label = self._create_target(self.train_sample)\n",
    "        self.test_label = self._create_target(self.test_sample)\n",
    "        self.val_label = self._create_target(self.val_sample)\n",
    "        self.process_data()\n",
    "        \n",
    "    def process_data(self):\n",
    "        self._print_summary()\n",
    "        self._training_sets_array()\n",
    "        self._scale_data()\n",
    "        \n",
    "    def shuffle_data(self, dataset):\n",
    "        return shuffle(dataset).reset_index(drop=True)\n",
    "    \n",
    "    def _split_test_data(self):\n",
    "        return train_test_split(self.test_sample, test_size=.2)\n",
    "        \n",
    "    def _create_target(self, dataset):\n",
    "        return np.array(dataset.pop(self.target))\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        print(\"Training data shape:\", self.train_sample.shape)\n",
    "        print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        print(\"Validation data shape:\", self.val_sample.shape)\n",
    "        print(\"train_label length:\", self.train_label.shape[0])\n",
    "        print(\"test_label length:\", self.test_label.shape[0])\n",
    "        print(\"val_label length:\", self.val_label.shape[0])\n",
    "        \n",
    "    def _training_sets_array(self):\n",
    "        self.train_sample = np.array(self.train_sample)\n",
    "        self.test_sample = np.array(self.test_sample)\n",
    "        self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train_sample = scaler.fit_transform(self.train_sample)\n",
    "        self.test_sample = scaler.transform(self.test_sample)\n",
    "        self.val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "pp = PreProcessing(list_data, target)\n",
    "# right now, train_sample is copied, so the original train_sample is not modified. We might need to reverse\n",
    "# our change later when it comes to feeding the model our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create method that ingests selected model, runs through 90 epochs, saves each model, graphs auc results for all\n",
    "# 90 models. then scores each based on \n",
    "# best .1-.9 threshold, then selects the best threshold for each epoch, then tests the best incremental threshold\n",
    "# for all 90 models. Then selects the top model, top threshold, then saves the model as a tf file. Then load model\n",
    "# and score it with results. \n",
    "# \n",
    "# then run through each threshold and picks the\n",
    "# top 5 scoring models and then runs through each threshold, \n",
    "\n",
    "# create method to test each threshold .1-.9 and save all results to a dataframe\n",
    "# create method that reads dataframe and selects which has the highest auc roc score and returns the best \n",
    "# threshold\n",
    "# create a method to load the algorithm\n",
    "\n",
    "# create method using code block below that selects each threshold and creates the fine tuning set, which then\n",
    "# tests each and selects the perfect threshold to use\n",
    "# create method that saves the new model in tf SavedModel format\n",
    "# create method that creates results dataframe along with score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_score: 0.8644288146005411\n",
      "[[42207  3368]\n",
      " [  329  1339]]\n"
     ]
    }
   ],
   "source": [
    "# Final model WORKING\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "# predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append each val to list and then call order command to order the list \n",
    "# Method for fine tuning threshold\n",
    "list_tune_thres = []\n",
    "\n",
    "best_thres = .1\n",
    "temp_val_neg = .1\n",
    "temp_val_pos = .1\n",
    "for val in range(0,5):\n",
    "    temp_val_neg = temp_val_neg - .01\n",
    "    temp_val_pos = temp_val_pos + .01\n",
    "    list_tune_thres.append(round(temp_val_neg, 2))\n",
    "    list_tune_thres.append(round(temp_val_pos, 2))\n",
    "list_tune_thres.append(round(best_thres, 2))    \n",
    "    \n",
    "list_tune_thres.sort()\n",
    "list_tune_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use this in method to run scoring on \n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "\n",
    "list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "                  .35, .4, .45, .5, .55, .6]\n",
    "\n",
    "list_threshold = [.31, .32, .33, .34, .35, .36, \n",
    "                  .37, .38, .39, .4, .41, .42,.43,.44,.45,.46]\n",
    "for threshold in list_threshold:\n",
    "    y_pred_class = binarize(y_pred, threshold)#[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, y_pred_class, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    # list_auc_score.append(auc_score)\n",
    "    print('auc_score:', auc_score, threshold)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copied from other machine. \n",
    "# #8 inner .5 dropout .0001 l2\n",
    "# import tensorflow as tf\n",
    "# import numpy as np \n",
    "# neg, pos = np.bincount(train_sample['0'])\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "# loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "\n",
    "# ##### model begins below ##### \n",
    "# checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# epochs = 90\n",
    "# lr = .0001\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "# ### Testing\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "# ### testing ^\n",
    "\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190000 samples, validate on 11811 samples\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ./cp.ckpt/cp-0001.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-c6565330ecae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-c6565330ecae>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(self, model, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                   \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                   callbacks=[cp_callback])\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CURRENT\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.train_sample = pp.train_sample\n",
    "        self.train_label = pp.train_label\n",
    "        self.test_sample = pp.test_sample\n",
    "        self.test_label = pp.test_label\n",
    "        self.val_sample = pp.val_sample\n",
    "        self.val_label = pp.val_label\n",
    "    \n",
    "    def _checkpoint_path(self):\n",
    "        checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def _define_checkpoint(self):\n",
    "        checkpoint_path = self._checkpoint_path()\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                         monitor='val_auc', \n",
    "                                                         verbose=1,\n",
    "                                                         save_best_only=False, \n",
    "                                                         save_weights_only=True,\n",
    "                                                         mode='max',\n",
    "                                                         save_freq='epoch')\n",
    "        return cp_callback\n",
    "    \n",
    "    def create_model(self, model, metrics, lr, loss):\n",
    "        '''ingests and compiles model'''\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n",
    "                      loss=loss, metrics=metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit_model(self, model, epochs):\n",
    "        model = self._save_weights(model)\n",
    "        model.fit(self.train_sample, self.train_label, \n",
    "                  validation_data=(self.val_sample, self.val_label), \n",
    "                  batch_size=20, epochs=epochs, shuffle=True, verbose=2, \n",
    "                  workers=16, use_multiprocessing=True,\n",
    "                  callbacks=[cp_callback])\n",
    "    \n",
    "    def _save_weights(self, model):\n",
    "        checkpoint_path = self._checkpoint_path()\n",
    "        model.save_weights(checkpoint_path.format(epoch=0))\n",
    "        return model \n",
    "\n",
    "\n",
    "##### hard code below #####\n",
    "\n",
    "df_train = pp.train_sample\n",
    "\n",
    "# define model parameters\n",
    "neg, pos = np.bincount(train_sample[target])\n",
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "LOSS = keras.losses.BinaryCrossentropy()\n",
    "LR = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "MODEL_SEQ = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(df_train.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(df_train.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "mod = Model()\n",
    "model = mod.create_model(MODEL_SEQ, metrics, lr, LOSS)\n",
    "mod.fit_model(model, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0000 auc_score: 0.4975001808804024\n",
      "epoch/checkpoint: 0001 auc_score: 0.7726566440521783\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0002 auc_score: 0.8183964721761776\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0003 auc_score: 0.8288777153655597\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0004 auc_score: 0.8332861572130622\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0005 auc_score: 0.8298790264074996\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0006 auc_score: 0.8274438566065901\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0007 auc_score: 0.8375698857576265\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0008 auc_score: 0.8086915923541561\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0009 auc_score: 0.8511602974466181\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0010 auc_score: 0.835950879507413\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0011 auc_score: 0.8312121766738453\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0012 auc_score: 0.8418760408741648\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0013 auc_score: 0.8396124639002336\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0014 auc_score: 0.8249947099788957\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0015 auc_score: 0.8370610045623429\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0016 auc_score: 0.8423824215683741\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0017 auc_score: 0.8425126619529082\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0018 auc_score: 0.8234455618833748\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "class TuneNeuralNet():\n",
    "    def __init__(self):\n",
    "        self.checkpoint_path = './cp.ckpt/'\n",
    "        self.list_auc_score = []\n",
    "        self.list_epoch = []\n",
    "    \n",
    "    def create_chkpt_list(self):\n",
    "        \n",
    "        list_checkpoints = []\n",
    "        for file in os.listdir(self.checkpoint_path):\n",
    "            val_checkpoint = file[0:12]\n",
    "            if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "                list_checkpoints.append(val_checkpoint)\n",
    "        list_checkpoints.sort()\n",
    "        return list_checkpoints\n",
    "    \n",
    "    def score_chkpts(self):\n",
    "        list_checkpoints = self.create_chkpt_list()\n",
    "        list_epoch = []\n",
    "\n",
    "        for chkpt in list_checkpoints:\n",
    "            filepath_chkpt = self.checkpoint_path + chkpt\n",
    "            model = mod.create_model(MODEL_SEQ, metrics, lr, loss)\n",
    "            model.load_weights(filepath_chkpt)\n",
    "            rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "            auc_score = self._calc_chkpt_score(rounded_predictions)\n",
    "            self._print_summary(chkpt, auc_score)\n",
    "            self._plot_auc_scores()\n",
    "#             self._confusion_matrix(rounded_predictions) #not needed for now \n",
    "        self._plot_auc_scores()\n",
    "\n",
    "            \n",
    "            \n",
    "    def _calc_chkpt_score(self, rounded_predictions):\n",
    "        fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        self.list_auc_score.append(auc_score)\n",
    "        return auc_score\n",
    "    \n",
    "    def _print_summary(self, chkpt, auc_score):\n",
    "        chkpt_num = chkpt[3:7]\n",
    "        print('epoch/checkpoint:', chkpt_num, 'auc_score:', auc_score)  \n",
    "    \n",
    "    def _confusion_matrix(self, rounded_predictions):\n",
    "        '''not in use at the moment'''\n",
    "        cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "    def _plot_auc_scores(self):\n",
    "        list_epoch = []\n",
    "        for val in range(1, len(self.list_auc_score) + 1):\n",
    "            list_epoch.append(val)\n",
    "        plt.plot(list_epoch, self.list_auc_score)\n",
    "        \n",
    "# NEXT, we need to add the plot\n",
    "list_epoch = []\n",
    "for val in range(1, len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "plt.plot(list_epoch, list_auc_score)\n",
    "\n",
    "\n",
    "tm = TuneNeuralNet()\n",
    "list_checkpoints = tm.create_chkpt_list()\n",
    "# print(list_checkpoints[0:1])\n",
    "tm.score_chkpts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = './cp.ckpt/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "checkpoint_path = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = checkpoint_path + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "    model.load_weights(filepath_chkpt)\n",
    "\n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    \n",
    "    chkpt_num = chkpt[3:7]\n",
    "    print('epoch/checkpoint:', chkpt_num, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1, len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURRENT!\n",
    "# This is the code we are using to create our model class. \n",
    "# NEXT, should we load model weights and use this as our method? \n",
    "# should we just load the model? Well, we do want to show them all our details, so we should do this all.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np \n",
    "\n",
    "# neg, pos = np.bincount(train_sample['0'])\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "# creating model\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "# #                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "# #                           keras.layers.Dropout(.5),\n",
    "# #                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "# saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# epochs = 2\n",
    "# # fitting model\n",
    "# model.fit(pp.train_sample, pp.train_label, validation_data=(pp.val_sample, pp.val_label), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "# NEXT, create a class that run through each model, threshold and tiny threshold \n",
    "\n",
    "checkpoint_path = './cp.ckpt/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/checkpoint: 0000 auc_score: 0.5012325521854574\n",
      "epoch/checkpoint: 0001 auc_score: 0.8108817325341947\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch/checkpoint: 0002 auc_score: 0.8183964721761776\n",
      "epoch/checkpoint: 0003 auc_score: 0.8288777153655597\n",
      "epoch/checkpoint: 0004 auc_score: 0.8332861572130622\n",
      "epoch/checkpoint: 0005 auc_score: 0.8298790264074996\n",
      "epoch/checkpoint: 0006 auc_score: 0.8274438566065901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-9ad92dc92fe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrounded_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mauc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CURRENT!\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "    model.load_weights(filepath_chkpt)\n",
    "\n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    \n",
    "    chkpt_num = chkpt[3:7]\n",
    "    print('epoch/checkpoint:', chkpt_num, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 auc_score: 0.49883807235657746\n",
      "epoch: 2 auc_score: 0.7928097864286312\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "epoch: 3 auc_score: 0.8104485288415588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15db984d0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    \n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model - for non OOP dataset\n",
    "list_auc_score = []\n",
    "path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "auc_score = auc(fpr, tpr)\n",
    "list_auc_score.append(auc_score)\n",
    "# val_epoch = val_epoch + 1\n",
    "print('auc_score:', auc_score)    \n",
    "cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                              keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 55\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# creating Modelcheckpoint\n",
    "checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 monitor='val_auc', \n",
    "                                                 verbose=1,\n",
    "                                                 save_best_only=False, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 mode='max',\n",
    "                                                 save_freq='epoch')\n",
    "\n",
    "\n",
    "# creating model\n",
    "epochs = 10\n",
    "lr = .0001\n",
    "metrics = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "#       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# saving weights for checkpoints\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# fitting model\n",
    "model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "          batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# now that all epochs have completed, we will test load and test each weight. \n",
    "# parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# a list of all model weights to parse through for testing results against the test set. \n",
    "checkpoint_path = './cp.ckpt.testing2/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "val_epoch = 0\n",
    "\n",
    "for chkpt in list_checkpoints:\n",
    "    filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "                          keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "                                             bias_regularizer=l2(0.0001),\n",
    "                                             activity_regularizer=l2(0.0001),\n",
    "#                                              kernel_constraint=l2(0.0001),\n",
    "#                                              bias_constraint=l2(0.0001)\n",
    "                                            ),\n",
    "                          keras.layers.Dropout(.5),\n",
    "                          keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "    model.load_weights(filepath_chkpt)\n",
    "    predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    val_epoch = val_epoch + 1\n",
    "    print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "    cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1,len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "    \n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
