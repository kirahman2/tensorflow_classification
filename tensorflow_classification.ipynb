{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (190000, 233)\n",
      "Testing data shape: (47243, 233)\n",
      "Validation data shape: (11811, 233)\n",
      "train_label length: 190000\n",
      "test_label length: 47243\n",
      "val_label length: 11811\n"
     ]
    }
   ],
   "source": [
    "# smote and upsampling + validation set pulled from training set (now we have data leakage)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "filepath_data = '/Users/krahman/work/tutorials/tensorflow_classification/data/'\n",
    "\n",
    "df_raw = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1) #del once finished testing\n",
    "\n",
    "train_sample = pd.read_csv(filepath_data + 'mod_x_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_label = pd.read_csv(filepath_data + 'mod_y_train.csv').drop('Unnamed: 0', axis=1)\n",
    "train_sample = pd.concat([train_sample, train_label], axis=1)\n",
    "test_sample = pd.read_csv(filepath_data + 'mod_x_test.csv').drop('Unnamed: 0', axis=1)\n",
    "test_label = pd.read_csv(filepath_data + 'mod_y_test.csv').drop('index', axis=1)\n",
    "test_sample = pd.concat([test_sample, test_label], axis=1)\n",
    "\n",
    "target = '0'\n",
    "\n",
    "list_data = [train_sample, test_sample]\n",
    "\n",
    "\n",
    "class PreProcessing():\n",
    "    def __init__(self, list_data, target):\n",
    "        self.target = target\n",
    "        self.train_sample = list_data[0].copy()\n",
    "        self.test_sample = list_data[1].copy()\n",
    "        self.test_sample = self.shuffle_data(self.test_sample)\n",
    "        self.test_sample, self.val_sample = self._split_test_data()\n",
    "        self.train_label = self._create_target(self.train_sample)\n",
    "        self.test_label = self._create_target(self.test_sample)\n",
    "        self.val_label = self._create_target(self.val_sample)\n",
    "        self.process_data()\n",
    "        \n",
    "    def process_data(self):\n",
    "        self._print_summary()\n",
    "        self._training_sets_array()\n",
    "        self._scale_data()\n",
    "        \n",
    "    def shuffle_data(self, dataset):\n",
    "        return shuffle(dataset).reset_index(drop=True)\n",
    "    \n",
    "    def _split_test_data(self):\n",
    "        return train_test_split(self.test_sample, test_size=.2)\n",
    "        \n",
    "    def _create_target(self, dataset):\n",
    "        return np.array(dataset.pop(self.target))\n",
    "        \n",
    "    def _print_summary(self):\n",
    "        print(\"Training data shape:\", self.train_sample.shape)\n",
    "        print(\"Testing data shape:\", self.test_sample.shape)\n",
    "        print(\"Validation data shape:\", self.val_sample.shape)\n",
    "        print(\"train_label length:\", self.train_label.shape[0])\n",
    "        print(\"test_label length:\", self.test_label.shape[0])\n",
    "        print(\"val_label length:\", self.val_label.shape[0])\n",
    "        \n",
    "    def _training_sets_array(self):\n",
    "        self.train_sample = np.array(self.train_sample)\n",
    "        self.test_sample = np.array(self.test_sample)\n",
    "        self.val_sample = np.array(self.val_sample)\n",
    "        \n",
    "    def _scale_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.train_sample = scaler.fit_transform(self.train_sample)\n",
    "        self.test_sample = scaler.transform(self.test_sample)\n",
    "        self.val_sample = scaler.transform(self.val_sample)\n",
    "\n",
    "pp = PreProcessing(list_data, target)\n",
    "# right now, train_sample is copied, so the original train_sample is not modified. We might need to reverse\n",
    "# our change later when it comes to feeding the model our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_model method is set to false.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "class CreateModel():\n",
    "    '''creates the model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.filepath_chkpt = './cp.ckpt/'\n",
    "        self.train_sample = pp.train_sample\n",
    "        self.train_label = pp.train_label\n",
    "        self.test_sample = pp.test_sample\n",
    "        self.test_label = pp.test_label\n",
    "        self.val_sample = pp.val_sample\n",
    "        self.val_label = pp.val_label\n",
    "    \n",
    "    def _checkpoint_path(self):\n",
    "        checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def _define_checkpoint(self):\n",
    "        checkpoint_path = self._checkpoint_path()\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                         monitor='val_auc', \n",
    "                                                         verbose=1,\n",
    "                                                         save_best_only=False, \n",
    "                                                         save_weights_only=True,\n",
    "                                                         mode='max',\n",
    "                                                         save_freq='epoch')\n",
    "        return cp_callback\n",
    "    \n",
    "    def create_model(self, model, metrics, lr, loss):\n",
    "        'ingests and compiles model'\n",
    "        model.compile(optimizer=keras.optimizers.Adam(lr=lr), \n",
    "                      loss=loss, metrics=metrics)\n",
    "        return model\n",
    "    \n",
    "    def fit_model(self, model, epochs, fit_model=False):\n",
    "        if fit_model:\n",
    "            model = self._save_weights(model)\n",
    "            model.fit(self.train_sample, self.train_label, \n",
    "                      validation_data=(self.val_sample, self.val_label), \n",
    "                      batch_size=20, epochs=epochs, shuffle=True, verbose=2, \n",
    "                      workers=16, use_multiprocessing=True,\n",
    "                      callbacks=[cp_callback])\n",
    "            \n",
    "        if ~fit_model:\n",
    "            print(\"fit_model method is set to false.\")\n",
    "    \n",
    "    def _save_weights(self, model):\n",
    "        checkpoint_path = self._checkpoint_path()\n",
    "        model.save_weights(checkpoint_path.format(epoch=0))\n",
    "        return model\n",
    "    \n",
    "mod = CreateModel()\n",
    "###########################\n",
    "##### hard code below #####\n",
    "\n",
    "DF_TRAIN = pp.train_sample\n",
    "\n",
    "##### Defining Model Parameters #####\n",
    "neg, pos = np.bincount(train_sample[target])\n",
    "initial_bias = np.log([pos/neg])\n",
    "OUTPUT_BIAS = tf.keras.initializers.Constant(initial_bias)\n",
    "LOSS = keras.losses.BinaryCrossentropy()\n",
    "LR = .0001\n",
    "METRICS = [keras.metrics.AUC(name='auc'),\n",
    "           keras.metrics.FalsePositives(name='fp'),\n",
    "           keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "MODEL_SEQ = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "                              keras.layers.Dropout(.5),\n",
    "                              keras.layers.Dense(1, activation='sigmoid', bias_initializer=OUTPUT_BIAS)])\n",
    "\n",
    "# MODEL_SEQ = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(DF_TRAIN.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=OUTPUT_BIAS)])\n",
    "\n",
    "\n",
    "model = mod.create_model(MODEL_SEQ, METRICS, LR, LOSS)\n",
    "mod.fit_model(model, epochs=100, fit_model=False) #comment out until next fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd\n",
    "\n",
    "class TuneThreshold():\n",
    "    '''Fine tuning each epoch\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.current_thres = []\n",
    "        self.top_score = []\n",
    "        self.df_all_results = pd.DataFrame()\n",
    "        self.best_chkpt = []\n",
    "        self.best_threshold = []\n",
    "        \n",
    "    def tune_threshold(self, model):\n",
    "        y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "        \n",
    "        list_threshold = [.05, .1, .15, .2, .25, .3, .35, .4, .45, \n",
    "                          .5, .55, .6, .65, .7, .75, .8, .85, .9]\n",
    "        self._tune_thres_methods(list_threshold, y_pred)\n",
    "        list_tune_thres = self._fine_tune_thres()\n",
    "        self._tune_thres_methods(list_tune_thres, y_pred)\n",
    "        list_tune_thres = self._fine_tune_thres_2()\n",
    "        self._tune_thres_methods(list_tune_thres, y_pred)        \n",
    "        self._create_results_df()\n",
    "        self._save_best_thres()\n",
    "    \n",
    "        \n",
    "    def _tune_thres_methods(self, list_threshold, y_pred):\n",
    "        list_auc_score = self._calc_thres_score(list_threshold, y_pred)        \n",
    "        df_results = self._create_df_results(list_auc_score, list_threshold)\n",
    "        self._calc_best_score(df_results)\n",
    "        \n",
    "    def _calc_thres_score(self, list_threshold, y_pred):\n",
    "        list_auc_score = []\n",
    "        for thres in list_threshold:\n",
    "            y_pred_class = binarize(y_pred, thres)\n",
    "            fpr, tpr, thresholds = roc_curve(pp.test_label, \n",
    "                                             y_pred_class, \n",
    "                                             pos_label=1)\n",
    "            auc_score = auc(fpr, tpr)\n",
    "            list_auc_score.append(auc_score)\n",
    "        return list_auc_score\n",
    "    \n",
    "    def _create_df_results(self, list_auc_score, list_threshold):\n",
    "        col_thres = pd.Series(list_threshold, name='current_thres')\n",
    "        col_auc = pd.Series(list_auc_score, name='auc_score')\n",
    "        df_results = pd.concat([col_thres, col_auc], axis=1)\n",
    "        return df_results\n",
    "    \n",
    "    def _calc_best_score(self, df_results):\n",
    "        val_max_auc = df_results.auc_score.max()\n",
    "        df_max_auc = df_results[df_results.auc_score == val_max_auc]\n",
    "        df_max_auc_index = df_max_auc.index[0]\n",
    "        self.current_thres = df_results.loc[df_max_auc_index, 'current_thres']\n",
    "        self.top_score = val_max_auc \n",
    "    \n",
    "    def _fine_tune_thres(self):\n",
    "        list_tune_thres = []\n",
    "        temp_val_neg = self.current_thres\n",
    "        temp_val_pos = self.current_thres\n",
    "        for val in range(0,5):\n",
    "            temp_val_neg = temp_val_neg - .01\n",
    "            temp_val_pos = temp_val_pos + .01\n",
    "            list_tune_thres.append(round(temp_val_neg, 2))\n",
    "            list_tune_thres.append(round(temp_val_pos, 2))\n",
    "        list_tune_thres.append(round(self.current_thres, 2))    \n",
    "        list_tune_thres.sort()\n",
    "        return list_tune_thres\n",
    "    \n",
    "    def _fine_tune_thres_2(self):\n",
    "        list_tune_thres = []\n",
    "        temp_val_neg = self.current_thres\n",
    "        temp_val_pos = self.current_thres\n",
    "        for val in range(0,5):\n",
    "            temp_val_neg = temp_val_neg - .001\n",
    "            temp_val_pos = temp_val_pos + .001\n",
    "            list_tune_thres.append(round(temp_val_neg, 3))\n",
    "            list_tune_thres.append(round(temp_val_pos, 3))\n",
    "        list_tune_thres.append(round(self.current_thres, 3))    \n",
    "        list_tune_thres.sort()\n",
    "        return list_tune_thres\n",
    "    \n",
    "    def _create_results_df(self):\n",
    "        col_chkpt = pd.Series(sc.chkpt_num, name='chkpt_epoch')\n",
    "        col_thres = pd.Series(self.current_thres, name='best_threshold')\n",
    "        col_auc = pd.Series(self.top_score, name='auc_score')\n",
    "        df_temp = pd.concat([col_chkpt, col_thres, col_auc], axis=1)\n",
    "        print(df_temp)\n",
    "        self.df_all_results = pd.concat([self.df_all_results, df_temp], axis=0)\n",
    "        \n",
    "    def _save_best_thres(self):\n",
    "        dataset = self.df_all_results.copy()\n",
    "        val_best_auc = dataset.auc_score.max()\n",
    "        val_best_chkpt = dataset[dataset.auc_score==val_best_auc].chkpt_epoch\n",
    "        val_best_thres = dataset[dataset.auc_score==val_best_auc].best_threshold\n",
    "        self.best_chkpt = val_best_chkpt[0]\n",
    "        self.best_threshold = val_best_thres[0]\n",
    "\n",
    "\n",
    "tt = TuneThreshold()\n",
    "# tt.tune_threshold(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  chkpt_epoch  best_threshold  auc_score\n",
      "0        0067           0.168   0.864589\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "  chkpt_epoch  best_threshold  auc_score\n",
      "0        0068           0.117   0.860955\n",
      "  chkpt_epoch  best_threshold  auc_score\n",
      "0        0067           0.168   0.864589\n",
      "0        0068           0.117   0.860955\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ScoreCheckpoints():\n",
    "    '''Loading and testing each checkpoint\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.checkpoint_path = './cp.ckpt/'\n",
    "        self.list_auc_score = []\n",
    "        self.list_epoch = []\n",
    "        self.chkpt_num = []\n",
    "    \n",
    "    def create_chkpt_list(self):\n",
    "        list_checkpoints = []\n",
    "        for file in os.listdir(self.checkpoint_path):\n",
    "            val_checkpoint = file[0:12]\n",
    "            if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "                list_checkpoints.append(val_checkpoint)\n",
    "        list_checkpoints.sort()\n",
    "        return list_checkpoints\n",
    "    \n",
    "    def score_chkpts(self):\n",
    "        list_checkpoints = self.create_chkpt_list()\n",
    "        list_epoch = []\n",
    "        for chkpt in list_checkpoints[67:69]: # setting 66:67 for fast testing\n",
    "            filepath_chkpt = self.checkpoint_path + chkpt\n",
    "            model = mod.create_model(MODEL_SEQ, METRICS, LR, LOSS)\n",
    "            model.load_weights(filepath_chkpt)\n",
    "            rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "            auc_score = self._calc_chkpt_score(rounded_predictions)\n",
    "            \n",
    "#             self._print_summary(chkpt, auc_score)\n",
    "            self.chkpt_num = chkpt[3:7]\n",
    "            \n",
    "            tt.tune_threshold(model)\n",
    "            self._plot_auc_scores()\n",
    "        # printing final dataframe results\n",
    "        print(tt.df_all_results)\n",
    "        self._plot_auc_scores()\n",
    "            \n",
    "    def _calc_chkpt_score(self, rounded_predictions):\n",
    "        fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "        # cm = confusion_matrix(pp.test_label, rounded_predictions) #not needed for now \n",
    "        auc_score = auc(fpr, tpr)\n",
    "        self.list_auc_score.append(auc_score)\n",
    "        return auc_score\n",
    "    \n",
    "#     def _print_summary(self, chkpt, auc_score):\n",
    "        # delete this when ready. We've added this to \n",
    "#         self.chkpt_num = chkpt[3:7]\n",
    "    \n",
    "    def _plot_auc_scores(self):\n",
    "        list_epoch = []\n",
    "        for val in range(1, len(self.list_auc_score) + 1):\n",
    "            list_epoch.append(val)\n",
    "        plt.plot(list_epoch, self.list_auc_score)\n",
    "        plt.title(\"Checkpoint Versus AUC Score\")\n",
    "        plt.ylabel(\"AUC Score\")\n",
    "        plt.xlabel(\"Checkpoint / Epoch\")\n",
    "\n",
    "sc = ScoreCheckpoints()\n",
    "list_checkpoints = sc.create_chkpt_list()\n",
    "sc.score_chkpts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "WARNING:tensorflow:From /Users/krahman/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./cp.ckpt/saved_model/assets\n",
      "\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               59904     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 586,497\n",
      "Trainable params: 586,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "auc for best model and best threshold: 0.8645888901808467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# NEXT, we need to create functions that we call. Test this with jupyter notebooks. \n",
    "# though do a spot check and see if everything appears to be in right order. move your code blocks around.\n",
    "\n",
    "class SaveModel():\n",
    "    def __init__(self):\n",
    "        self.chkpt_file_path = self._create_file_path()\n",
    "        self.saved_model_file_path = mod.filepath_chkpt + 'saved_model/'\n",
    "    \n",
    "    def _create_file_path(self):\n",
    "        chkpt_file_path = mod.filepath_chkpt + 'cp-' + tt.best_chkpt + '.ckpt'\n",
    "        return chkpt_file_path\n",
    "    \n",
    "    def save_best_model(self):\n",
    "        model = self._load_model_weights()\n",
    "        save_model_file_path = mod.filepath_chkpt + 'saved_model/'\n",
    "        tf.saved_model.save(model, self.saved_model_file_path)\n",
    "    \n",
    "    def _load_model_weights(self):\n",
    "        model = mod.create_model(MODEL_SEQ, METRICS, LR, LOSS)\n",
    "        model.load_weights(self.chkpt_file_path)\n",
    "        return model\n",
    "        \n",
    "    def load_saved_model(self, summary=False):\n",
    "        model = tf.keras.models.load_model(self.saved_model_file_path)\n",
    "        if summary:\n",
    "            print('\\n')\n",
    "            print(model.summary())\n",
    "            print('\\n')\n",
    "        return model\n",
    "\n",
    "    def predict_binarize(self, model, test_data):\n",
    "        y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "        y_pred_class = binarize(y_pred, tt.best_threshold)\n",
    "        fpr, tpr, thresholds = roc_curve(pp.test_label, y_pred_class, pos_label=1)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        print('\\nauc for best model and best threshold:', auc_score)\n",
    "        return y_pred_class\n",
    "        \n",
    "    \n",
    "sm = SaveModel()\n",
    "sm.save_best_model()\n",
    "model_1 = sm.load_saved_model(summary=True)\n",
    "y_pred = sm.predict_binarize(model_1, pp.test_sample)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-024601f11756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#     model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SEQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOSS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_path = './cp.ckpt/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()\n",
    "\n",
    "checkpoint_path = './cp.ckpt/'\n",
    "list_auc_score = []\n",
    "list_epoch = []\n",
    "\n",
    "for chkpt in list_checkpoints[65:70]: #testing 65:70 to keep cycles quick for testing.\n",
    "    filepath_chkpt = checkpoint_path + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "#     model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dropout(.5),\n",
    "#                               keras.layers.Dense(1, activation='sigmoid', bias_initializer=OUTPUT_BIAS)])\n",
    "    \n",
    "#     model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    \n",
    "    mod = Model()\n",
    "    model = mod.create_model(MODEL_SEQ, METRICS, LR, LOSS)\n",
    "\n",
    "    model.load_weights(filepath_chkpt)\n",
    "\n",
    "    predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "    rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    list_auc_score.append(auc_score)\n",
    "    \n",
    "    chkpt_num = chkpt[3:7]\n",
    "    print('epoch / checkpoint:', chkpt_num, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "list_epoch = []\n",
    "for val in range(1, len(list_auc_score) + 1):\n",
    "    list_epoch.append(val)\n",
    "\n",
    "plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this in method to run scoring on \n",
    "\n",
    "# append each val to list and then call order command to order the list \n",
    "# Method for fine tuning threshold\n",
    "\n",
    "# we need to score each threshold for each epoch/checkpoint, then select the highest auc score, then select\n",
    "# the corresponding threshold, then take that .x threshold and calculate the correct list of fine tuned\n",
    "# thresholds to test, then test the fine tune threshold and select the highest auc score and corresponding\n",
    "# threshold. During this entire time, we need to store the epoch #, and every threshold score in a \n",
    "# pandas dataframe, then return the highest auc roc score along with the best threshold to use and print that\n",
    "# info out, then automatically store and save the best model in SavedModel / .tf method. \n",
    "\n",
    "list_tune_thres = []\n",
    "\n",
    "best_thres = .1\n",
    "temp_val_neg = .1\n",
    "temp_val_pos = .1\n",
    "for val in range(0,5):\n",
    "    temp_val_neg = temp_val_neg - .01\n",
    "    temp_val_pos = temp_val_pos + .01\n",
    "    list_tune_thres.append(round(temp_val_neg, 2))\n",
    "    list_tune_thres.append(round(temp_val_pos, 2))\n",
    "list_tune_thres.append(round(best_thres, 2))    \n",
    "    \n",
    "list_tune_thres.sort()\n",
    "list_tune_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "\n",
    "list_threshold = [.05, .1, .15, .2, .25, .3, \n",
    "                  .35, .4, .45, .5, .55, .6, \n",
    "                  .65, .7, .75, .8, .85, .9]\n",
    "\n",
    "# list_threshold = [.31, .32, .33, .34, .35, .36, \n",
    "#                   .37, .38, .39, .4, .41, .42,.43,.44,.45,.46]\n",
    "\n",
    "for threshold in list_threshold:\n",
    "    y_pred_class = binarize(y_pred, threshold)#[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(pp.test_label, y_pred_class, pos_label=1)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    # list_auc_score.append(auc_score)\n",
    "    print('auc_score:', auc_score, threshold)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CURRENT!\n",
    "# This is the code we are using to create our model class. \n",
    "# NEXT, should we load model weights and use this as our method? \n",
    "# should we just load the model? Well, we do want to show them all our details, so we should do this all.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np \n",
    "\n",
    "# neg, pos = np.bincount(train_sample['0'])\n",
    "# initial_bias = np.log([pos/neg])\n",
    "# output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "\n",
    "# loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# checkpoint_path = \"./cp.ckpt/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "# creating model\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "# #                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "# #                           keras.layers.Dropout(.5),\n",
    "# #                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "# saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# epochs = 2\n",
    "# # fitting model\n",
    "# model.fit(pp.train_sample, pp.train_label, validation_data=(pp.val_sample, pp.val_label), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "# NEXT, create a class that run through each model, threshold and tiny threshold \n",
    "\n",
    "checkpoint_path = './cp.ckpt/'\n",
    "list_checkpoints = []\n",
    "for file in os.listdir(checkpoint_path):\n",
    "    val_checkpoint = file[0:12]\n",
    "    if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "        list_checkpoints.append(val_checkpoint)\n",
    "list_checkpoints.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CURRENT!\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "# #     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "# #                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "# #                           keras.layers.Dropout(.5),\n",
    "# #                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "#     model = keras.Sequential([keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dropout(.5),\n",
    "#                               keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "    \n",
    "#     model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "#     model.load_weights(filepath_chkpt)\n",
    "\n",
    "#     predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "    \n",
    "#     chkpt_num = chkpt[3:7]\n",
    "#     print('epoch/checkpoint:', chkpt_num, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "\n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(pp.train_sample.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "    \n",
    "#     predictions = model.predict(pp.test_sample, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(pp.test_sample, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(pp.test_label, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(pp.test_label, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "\n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final model - for non OOP dataset\n",
    "# list_auc_score = []\n",
    "# path_savedmodel = '/Users/krahman/work/tutorials/tensorflow_classification/saved_models_2/'\n",
    "# model = tf.keras.models.load_model(path_savedmodel)\n",
    "\n",
    "# predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "# rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "# fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "# auc_score = auc(fpr, tpr)\n",
    "# list_auc_score.append(auc_score)\n",
    "# # val_epoch = val_epoch + 1\n",
    "# print('auc_score:', auc_score)    \n",
    "# cm = confusion_matrix(test_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # creating Modelcheckpoint\n",
    "# checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "\n",
    "# # creating model\n",
    "# epochs = 10\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "# #       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "# #       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# # saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# # fitting model\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# # now that all epochs have completed, we will test load and test each weight. \n",
    "# # parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# # a list of all model weights to parse through for testing results against the test set. \n",
    "# checkpoint_path = './cp.ckpt.testing2/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                               keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dropout(.5),\n",
    "#                               keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "#     predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "    \n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # creating Modelcheckpoint\n",
    "# checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "\n",
    "# # creating model\n",
    "# epochs = 10\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "# #                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "# #       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "# #       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# # saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# # fitting model\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# # now that all epochs have completed, we will test load and test each weight. \n",
    "# # parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# # a list of all model weights to parse through for testing results against the test set. \n",
    "# checkpoint_path = './cp.ckpt.testing2/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                               keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),bias_regularizer=l2(0.0001)),\n",
    "#                               keras.layers.Dropout(.5),\n",
    "#                               keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "#     predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "    \n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # creating Modelcheckpoint\n",
    "# checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "\n",
    "# # creating model\n",
    "# epochs = 55\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "# #       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "# #       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# # saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# # fitting model\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# # now that all epochs have completed, we will test load and test each weight. \n",
    "# # parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# # a list of all model weights to parse through for testing results against the test set. \n",
    "# checkpoint_path = './cp.ckpt.testing2/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "#     predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "    \n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # creating Modelcheckpoint\n",
    "# checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "\n",
    "# # creating model\n",
    "# epochs = 10\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "# #       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "# #       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# # saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# # fitting model\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# # now that all epochs have completed, we will test load and test each weight. \n",
    "# # parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# # a list of all model weights to parse through for testing results against the test set. \n",
    "# checkpoint_path = './cp.ckpt.testing2/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "#     predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "    \n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# # creating Modelcheckpoint\n",
    "# checkpoint_path = \"./cp.ckpt.testing2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                                                  monitor='val_auc', \n",
    "#                                                  verbose=1,\n",
    "#                                                  save_best_only=False, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  mode='max',\n",
    "#                                                  save_freq='epoch')\n",
    "\n",
    "\n",
    "# # creating model\n",
    "# epochs = 10\n",
    "# lr = .0001\n",
    "# metrics = [keras.metrics.AUC(name='auc'),\n",
    "#            keras.metrics.FalsePositives(name='fp'),\n",
    "#            keras.metrics.FalseNegatives(name='fn')]\n",
    "\n",
    "# model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "# # Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "# #       bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "# #       activity_regularizer=None, kernel_constraint=None, bias_constraint=None\n",
    "    \n",
    "# model.compile(optimizer=keras.optimizers.Adam(lr=lr), loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "# # saving weights for checkpoints\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "# # fitting model\n",
    "# model.fit(scaled_train_samples, train_labels, validation_data=(scaled_val_samples, val_labels), \n",
    "#           batch_size=20, epochs=epochs, shuffle=True, verbose=2, workers=16, \n",
    "#           use_multiprocessing=True,\n",
    "#           callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "# # now that all epochs have completed, we will test load and test each weight. \n",
    "# # parsing through folder with saved weights and selecting unique values with cp-0001 to cp-xxxx to create\n",
    "# # a list of all model weights to parse through for testing results against the test set. \n",
    "# checkpoint_path = './cp.ckpt.testing2/'\n",
    "# list_checkpoints = []\n",
    "# for file in os.listdir(checkpoint_path):\n",
    "#     val_checkpoint = file[0:12]\n",
    "#     if file[0:3]=='cp-' and val_checkpoint not in list_checkpoints:\n",
    "#         list_checkpoints.append(val_checkpoint)\n",
    "# list_checkpoints.sort()\n",
    "\n",
    "\n",
    "# # parsing through each saved weights through the list, then loading the weights, scoring and plotting results.\n",
    "# filepath_checkpoint_folder = './cp.ckpt.testing2/'\n",
    "# list_auc_score = []\n",
    "# list_epoch = []\n",
    "# val_epoch = 0\n",
    "\n",
    "# for chkpt in list_checkpoints:\n",
    "#     filepath_chkpt = filepath_checkpoint_folder + chkpt\n",
    "\n",
    "#     model = keras.Sequential([keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001), input_shape=(train_samples.shape[-1],)),\n",
    "#                           keras.layers.Dense(16, activation='relu', kernel_regularizer=l2(0.0001),\n",
    "#                                              bias_regularizer=l2(0.0001),\n",
    "#                                              activity_regularizer=l2(0.0001),\n",
    "# #                                              kernel_constraint=l2(0.0001),\n",
    "# #                                              bias_constraint=l2(0.0001)\n",
    "#                                             ),\n",
    "#                           keras.layers.Dropout(.5),\n",
    "#                           keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)])\n",
    "\n",
    "\n",
    "    \n",
    "#     model.load_weights(filepath_chkpt)\n",
    "#     predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     rounded_predictions = model.predict_classes(scaled_test_samples, batch_size=10, verbose=0)\n",
    "#     fpr, tpr, thresholds = roc_curve(test_labels, rounded_predictions, pos_label=1)\n",
    "#     auc_score = auc(fpr, tpr)\n",
    "#     list_auc_score.append(auc_score)\n",
    "#     val_epoch = val_epoch + 1\n",
    "#     print('epoch:', val_epoch, 'auc_score:', auc_score)    \n",
    "#     cm = confusion_matrix(test_labels, rounded_predictions)\n",
    "    \n",
    "# list_epoch = []\n",
    "# for val in range(1,len(list_auc_score) + 1):\n",
    "#     list_epoch.append(val)\n",
    "    \n",
    "# plt.plot(list_epoch, list_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
